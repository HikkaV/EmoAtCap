{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(2)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras_preprocessing import image as im\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(2)\n",
    "   np.random.seed(2)\n",
    "   random.seed(2)\n",
    "   torch.manual_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers import models\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/emo-at-cap/emo-at-cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'annotation', 'human_sentiment', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = df['annotation'].str.lower().values\n",
    "sentiment = df['human_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Positive', ..., 'Positive', 'Negative',\n",
       "       'Negative'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_sen = {'Negative' : 0, 'Neutral' : 1, 'Positive' : 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSA(nn.Module):\n",
    "    def __init__(self, finetune=False):\n",
    "        super(BertSA, self).__init__()\n",
    "        reset_random_seeds()\n",
    "        self.bert = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "        if not finetune:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        ### New layers:\n",
    "        self.linear1 = nn.Linear(768, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(256, 3) ## as you have 3 classes in the output\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoded = self.bert(input_ids,attention_mask=attention_mask)\n",
    "        encoded = mean_pooling(encoded, attention_mask)\n",
    "        linear1_out = self.relu(self.linear1(encoded))\n",
    "        linear2_out = self.linear2(linear1_out)\n",
    "        return linear2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertSA()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(annotations, sentiment , random_state=0, test_size=0.2)\n",
    "X_val, X_test, y_val , y_test = train_test_split(X_val,y_val , random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X,y, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mapping_sen =  {'Negative' : 0, 'Neutral' : 1, 'Positive' : 2}\n",
    "        X = self.map_X(list(X))\n",
    "        self.X, self.mask = X['input_ids'], X['attention_mask']\n",
    "        self.y = self.map_y(y)\n",
    "    \n",
    "    def map_X(self, X):\n",
    "        return self.tokenizer(X, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    def map_y(self, y):\n",
    "        return torch.tensor([self.mapping_sen[i] for i in y])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.X[idx], self.mask[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(X_train, y_train, tokenizer)\n",
    "validation_dataset = SentimentDataset(X_val, y_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSA(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss : 0.431018453091383, Training accuracy : 0.8486328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/25 [00:41<16:47, 41.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.431018453091383, Val accuracy : 0.8828125\n",
      "Training loss : 0.2864858365307252, Training accuracy : 0.9000651041666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/25 [01:23<15:59, 41.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.2864858365307252, Val accuracy : 0.8958333333333334\n",
      "Training loss : 0.24940494758387408, Training accuracy : 0.9147135416666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 3/25 [02:06<15:30, 42.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.24940494758387408, Val accuracy : 0.9010416666666666\n",
      "Training loss : 0.21579668732980886, Training accuracy : 0.9248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 4/25 [02:51<15:08, 43.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.21579668732980886, Val accuracy : 0.9036458333333334\n",
      "Training loss : 0.19703239823381105, Training accuracy : 0.931640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 5/25 [03:34<14:27, 43.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.19703239823381105, Val accuracy : 0.90625\n",
      "Training loss : 0.1725961590806643, Training accuracy : 0.9381510416666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 6/25 [04:15<13:29, 42.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.1725961590806643, Val accuracy : 0.90625\n",
      "Training loss : 0.1573308284084002, Training accuracy : 0.9495442708333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 7/25 [04:57<12:38, 42.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.1573308284084002, Val accuracy : 0.9088541666666666\n",
      "Training loss : 0.13726694975048304, Training accuracy : 0.9534505208333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 8/25 [05:37<11:46, 41.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.13726694975048304, Val accuracy : 0.9140625\n",
      "Training loss : 0.1243599842612942, Training accuracy : 0.95703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 9/25 [06:19<11:08, 41.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.1243599842612942, Val accuracy : 0.9114583333333334\n",
      "Training loss : 0.10526410272965829, Training accuracy : 0.9671223958333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 10/25 [07:00<10:20, 41.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.10526410272965829, Val accuracy : 0.9140625\n",
      "Training loss : 0.0922058407838146, Training accuracy : 0.9729817708333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 11/25 [07:39<09:32, 40.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.0922058407838146, Val accuracy : 0.9088541666666666\n",
      "Training loss : 0.0802367286135753, Training accuracy : 0.9749348958333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 12/25 [08:20<08:51, 40.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.0802367286135753, Val accuracy : 0.8984375\n",
      "Training loss : 0.07208625289301078, Training accuracy : 0.978515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 13/25 [09:02<08:14, 41.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.07208625289301078, Val accuracy : 0.921875\n",
      "Training loss : 0.06635037312904994, Training accuracy : 0.9811197916666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 14/25 [09:44<07:33, 41.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.06635037312904994, Val accuracy : 0.9036458333333334\n",
      "Training loss : 0.05806255231921872, Training accuracy : 0.9827473958333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 15/25 [10:26<06:56, 41.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.05806255231921872, Val accuracy : 0.8958333333333334\n",
      "Training loss : 0.04913043541212877, Training accuracy : 0.9853515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 16/25 [11:08<06:14, 41.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.04913043541212877, Val accuracy : 0.9088541666666666\n",
      "Training loss : 0.043987667420879006, Training accuracy : 0.990234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 17/25 [11:49<05:33, 41.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.043987667420879006, Val accuracy : 0.9036458333333334\n",
      "Training loss : 0.03701903965945045, Training accuracy : 0.9918619791666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 18/25 [12:31<04:52, 41.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.03701903965945045, Val accuracy : 0.9010416666666666\n",
      "Training loss : 0.03478809500423571, Training accuracy : 0.9925130208333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 19/25 [13:13<04:10, 41.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.03478809500423571, Val accuracy : 0.8958333333333334\n",
      "Training loss : 0.031094715697690845, Training accuracy : 0.9938151041666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 20/25 [13:55<03:28, 41.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.031094715697690845, Val accuracy : 0.8958333333333334\n",
      "Training loss : 0.0247239531405891, Training accuracy : 0.9957682291666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 21/25 [14:37<02:47, 41.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.0247239531405891, Val accuracy : 0.8932291666666666\n",
      "Training loss : 0.02178678335621953, Training accuracy : 0.9967447916666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 22/25 [15:19<02:05, 41.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.02178678335621953, Val accuracy : 0.90625\n",
      "Training loss : 0.02938339700146268, Training accuracy : 0.9915364583333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 23/25 [16:01<01:23, 41.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.02938339700146268, Val accuracy : 0.9036458333333334\n",
      "Training loss : 0.028957261859128874, Training accuracy : 0.9928385416666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 24/25 [16:43<00:42, 42.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.028957261859128874, Val accuracy : 0.8958333333333334\n",
      "Training loss : 0.020991014627118904, Training accuracy : 0.9967447916666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [17:23<00:00, 41.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss : 0.020991014627118904, Val accuracy : 0.90625\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "overall_loss = []\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "    tmp_train_loss = []\n",
    "    train_predictions = []\n",
    "    all_labels = []\n",
    "    model.train()\n",
    "    for train_idx in range(0, len(train_dataset), batch_size):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, mask, labels = train_dataset[train_idx:train_idx+batch_size]\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs, mask)\n",
    "        train_predictions.extend(np.argmax(outputs.cpu().detach().numpy(),axis=1))\n",
    "        loss_result = loss(outputs, labels)\n",
    "        loss_result.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        tmp_train_loss.append(loss_result.item())\n",
    "        \n",
    "    epoch_loss = np.mean(tmp_train_loss)\n",
    "    train_accuracy = accuracy_score(y_true=all_labels, y_pred=train_predictions)\n",
    "    \n",
    "    print('Training loss : {}, Training accuracy : {}'.format(epoch_loss, train_accuracy))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "         val_predictions = []\n",
    "         tmp_val_loss = []\n",
    "         all_labels = []\n",
    "\n",
    "         for val_idx in range(0, len(validation_dataset), batch_size):\n",
    "            inputs, mask, labels = validation_dataset[val_idx:val_idx+batch_size]\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "            inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "            outputs = model(inputs, mask)\n",
    "            \n",
    "            val_loss_result = loss(outputs, labels)\n",
    "            tmp_val_loss.append(val_loss_result)\n",
    "            \n",
    "            val_predictions.extend(np.argmax(outputs.cpu().detach().numpy(), axis=1))\n",
    "            \n",
    "         epoch_val_loss = np.mean(tmp_train_loss)\n",
    "         val_accuracy = accuracy_score(y_true=all_labels, y_pred=val_predictions)\n",
    "\n",
    "         print('Val loss : {}, Val accuracy : {}'.format(epoch_val_loss, val_accuracy))\n",
    "\n",
    "   \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentimentDataset(X_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.38545188307762146, Test accuracy : 0.921875\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_predictions = []\n",
    "tmp_test_loss = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "\n",
    "     for test_idx in range(0, len(test_dataset), batch_size):\n",
    "        inputs, mask, labels = test_dataset[test_idx:test_idx+batch_size]\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "\n",
    "        test_loss_result = loss(outputs, labels)\n",
    "        tmp_test_loss.append(test_loss_result)\n",
    "\n",
    "        test_predictions.extend(np.argmax(outputs.cpu().detach().numpy(), axis=1))\n",
    "\n",
    "     test_loss = np.mean(tmp_test_loss)\n",
    "     test_accuracy = accuracy_score(y_true=all_labels, y_pred=test_predictions)\n",
    "\n",
    "     print('Test loss : {}, Test accuracy : {}'.format(test_loss, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Negative': 0, 'Neutral': 1, 'Positive': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_sen_mapping = dict([(v,k) for k,v in mapping_sen.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sen_mapping, sentence):\n",
    "    sentence = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "        sentence = sentence.to('cpu')\n",
    "        label = sen_mapping[np.argmax(model(**sentence).cpu().detach().numpy())]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, inverse_sen_mapping, 'happy people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = ['sa_lstm_stbs']\n",
    "df['f1_macro'] = [f1_score(y_true=test_dataset.y.numpy(), y_pred=test_predictions, average='macro')]\n",
    "df['f1_weighted'] = [f1_score(y_true=test_dataset.y.numpy(), y_pred=test_predictions, average='weighted')]\n",
    "df['acc'] = [accuracy_score(y_true=test_dataset.y.numpy(), y_pred=test_predictions)]\n",
    "df.to_csv('sa_lstm_stbs_logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sa_lstm_stbs</td>\n",
       "      <td>0.850671</td>\n",
       "      <td>0.918409</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  f1_macro  f1_weighted       acc\n",
       "0  sa_lstm_stbs  0.850671     0.918409  0.921875"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
