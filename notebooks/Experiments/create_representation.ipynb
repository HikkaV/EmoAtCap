{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing import image as im\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertModel, BertTokenizer\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "try:\n",
    "    import tensorflow_text as text\n",
    "except:\n",
    "    pass\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.applications as apps\n",
    "import transformers\n",
    "sns.set()\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_path, annotations_path):\n",
    "    df = pd.read_csv(annotations_path)\n",
    "    df = df[~df['human_sentiment'].isna()]\n",
    "    in_folder = os.listdir(image_path)\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'] = \\\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'].apply(lambda x: os.path.join(image_path,x))\n",
    "    images = []\n",
    "    for image_name in df['image_name'].values:\n",
    "        images.append(np.array(im.load_img(image_name, target_size=(224,224))))\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "    annotations = df['annotation'].str.lower().values\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations = load_data(\"../../data/emo-at-cap/images/\", '../../data/emo-at-cap/emo-at-cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_folder('../image_features')\n",
    "check_folder('../text_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation_tensorflow(data, model, path, n_components=128, batch_size=16):\n",
    " \n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for sample in tqdm(data): \n",
    "            sample = np.expand_dims(sample,axis=0)\n",
    "            feature = model.predict(sample)\n",
    "            try:\n",
    "                feature = np.hstack(feature.numpy())\n",
    "            except:\n",
    "                feature = np.hstack(feature)\n",
    "            csv_writer.writerow(feature)            \n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data, path):\n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        csv_writer.writerows(data)            \n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet(block='conv4_block5_out', pooling=True):\n",
    "    resnet = apps.ResNet152V2(include_top=False, weights='imagenet')\n",
    "    outputs = [i for i in resnet.layers if i.name==block][0]\n",
    "    inputs = resnet.layers[0]\n",
    "    if pooling:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(outputs.output)\n",
    "    else:\n",
    "        x = tf.keras.layers.Flatten()(outputs.output)\n",
    "    resnet = tf.keras.Model(inputs.input,x)\n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_representation(data, path):\n",
    "    model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=False)\n",
    "    placeholder = tf.placeholder(tf.string, shape=[None]) \n",
    "    representation = model(placeholder, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    representation = tf.reduce_mean(representation, axis=1)\n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for sample in tqdm(data): \n",
    "                sample = np.expand_dims(sample,axis=0)\n",
    "                feature = sess.run(representation, feed_dict={placeholder: sample})\n",
    "                feature = np.hstack(feature)\n",
    "                csv_writer.writerow(feature)            \n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_from_disk(path):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_representation(data, model_name, path, gpu=False, \n",
    "                        batch_size=16,\n",
    "                       layer='emb', layer_num=11, num_last_layers_to_use=4):\n",
    "    assert layer in ['emb','intermidiate', 'sum'], 'Please choose name of layer from the following list: ({})'.format(*['emb','intermidiate','sum'])\n",
    "    \n",
    "    if layer!='sum':\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "   \n",
    "    if layer=='emb':\n",
    "        model = model.transformer.wte\n",
    "    elif layer=='intermidiate':\n",
    "        model_embs = model.get_input_embeddings()\n",
    "        model = model.transformer.h[layer_num]\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if gpu:\n",
    "        model.to('cuda')\n",
    "    else:\n",
    "        model.to('cpu')\n",
    "        \n",
    "    tokenized_data = []\n",
    "    for sample in data:\n",
    "        tokenized_data.append(np.array(tokenizer.encode(sample)))\n",
    "\n",
    "        \n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for tokenized in tqdm(tokenized_data):\n",
    "            tokenized = torch.tensor([tokenized])\n",
    "            \n",
    "            if gpu:\n",
    "                tokenized = tokenized.to('cuda')\n",
    "            else:\n",
    "                tokenized = tokenized.to('cpu')\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                if layer=='intermidiate':\n",
    "                    tokenized = model_embs(tokenized)\n",
    "                    outputs = model(tokenized)[0]\n",
    "                else:\n",
    "                    outputs = model(tokenized)\n",
    "                if layer=='sum':\n",
    "                    outputs = outputs.hidden_states[-num_last_layers_to_use:]\n",
    "                    features = torch.zeros_like(outputs[0])\n",
    "                    for hidden in outputs:\n",
    "                        features+=hidden\n",
    "                    outputs = features/num_last_layers_to_use\n",
    "                features = torch.mean(outputs, dim=1)\n",
    "            features = np.ravel(features.cpu().detach().numpy())\n",
    "            csv_writer.writerow(features)\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_representation(data, model_name, path, gpu=False, \n",
    "                        batch_size=16,\n",
    "                       layer='emb', layer_num=11, num_last_layers_to_use=4):\n",
    "    assert layer in ['emb','intermidiate', 'sum'], 'Please choose name of layer from the following list: ({})'.format(*['emb','intermidiate','sum'])\n",
    "    \n",
    "    model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "   \n",
    "    if layer=='emb':\n",
    "        model = model.get_input_embeddings()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "    if gpu:\n",
    "        model.to('cuda')\n",
    "    else:\n",
    "        model.to('cpu')\n",
    "        \n",
    "    tokenized_data = []\n",
    "    for sample in data:\n",
    "        tokenized_data.append(np.array(tokenizer.encode(sample)))\n",
    "\n",
    "        \n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for tokenized in tqdm(tokenized_data):\n",
    "            tokenized = torch.tensor([tokenized])\n",
    "            \n",
    "            if gpu:\n",
    "                tokenized = tokenized.to('cuda')\n",
    "            else:\n",
    "                tokenized = tokenized.to('cpu')\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                if layer=='intermidiate':\n",
    "                    outputs = model(tokenized)[2][layer_num]\n",
    "                elif layer=='emb':\n",
    "                    outputs = model(tokenized)\n",
    "                elif layer=='sum':\n",
    "                    outputs = model(tokenized)[2][-num_last_layers_to_use:]\n",
    "                    features = torch.zeros_like(outputs[0])\n",
    "                    for hidden in outputs:\n",
    "                        features+=hidden\n",
    "                    outputs = features/num_last_layers_to_use\n",
    "                features = torch.mean(outputs, dim=1)\n",
    "            features = np.ravel(features.cpu().detach().numpy())\n",
    "            csv_writer.writerow(features)\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "def prepare_annotations_embeddings(annotations):\n",
    "    prepared_annotations = []\n",
    "    for sentence in annotations:\n",
    "        sentence = ''.join([i for i in sentence if not (i in [',','.','!','?'] )])\n",
    "        prepared_annotations.append([i for i in sentence.split(' ') if not (i in stopwords.words('english'))])\n",
    "    unique_words_annotations = np.unique(np.hstack(prepared_annotations))\n",
    "    return prepared_annotations, unique_words_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation_embeddings(embeddings, sentences, path, dim=300):\n",
    "    with open(path, \"w\") as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for n, sentence in enumerate(sentences):\n",
    "            vector = np.zeros(shape=(dim,))\n",
    "            counter = 0 \n",
    "            for word in sentence:\n",
    "                representation = embeddings.get(word)\n",
    "                if not(representation is None):\n",
    "                    vector+=representation\n",
    "                    counter+=1\n",
    "            if counter!=0:\n",
    "                vector/=counter\n",
    "            csv_writer.writerow(vector)\n",
    "    print('Saved representation to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_rep(tokens, embeddings):\n",
    "    dict_tokens = {}\n",
    "    missing = []\n",
    "    for w in tqdm(tokens):\n",
    "        try:\n",
    "            try:\n",
    "                dict_tokens.update({w: embeddings.word_vec(w.lower())})\n",
    "            except:\n",
    "                dict_tokens.update({w: embeddings.word_vec(w.captialize())})\n",
    "        except:\n",
    "            missing.append(w)\n",
    "\n",
    "    print('{} words where absent in embedding'.format(len(missing)))\n",
    "    return dict_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_rep_glove(tokens, embeddings):\n",
    "    dict_tokens = {}\n",
    "    missing = []\n",
    "    for w in tqdm(tokens):\n",
    "        if w in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w]})\n",
    "        elif w.lower() in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w.lower()]})\n",
    "        elif w.capitalize() in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w.capitalize()]})\n",
    "        else:\n",
    "            missing.append(w)\n",
    "\n",
    "    print('{} words where absent in embedding'.format(len(missing)))\n",
    "    return dict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file, 'r')\n",
    "    glove_embeddings = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        glove_embeddings[word] = wordEmbedding\n",
    "    print(len(glove_embeddings), \" words loaded!\")\n",
    "    return glove_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_annotations, unique_words_annotations = prepare_annotations_embeddings(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "resnet = load_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:38<00:00, 48.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/resnet_conv4_block5.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, resnet, '../image_features/resnet_conv4_block5.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del resnet\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet conv5_block3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = load_resnet(block='conv5_block3_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [02:05<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/resnet_conv5_block3.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, resnet, '../image_features/resnet_conv5_block3.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2  word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:01<00:00, 1408.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/gpt2_wte.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_representation(data=prepared_annotations, model_name='gpt2-medium', path='../text_features/gpt2_wte.tsv',\n",
    "                   gpu=False, layer='emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2 11th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:08<00:00, 231.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/gpt2_11th_layer.tsv\n"
     ]
    }
   ],
   "source": [
    "gpt2_representation(data=prepared_annotations, model_name='gpt2-medium', path='../text_features/gpt2_11th_layer.tsv',\n",
    "                   gpu=False, layer='intermidiate', layer_num=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2 last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [00:08<00:00, 231.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/gpt2_last_layer.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_representation(data=prepared_annotations, model_name='gpt2-medium', path='../text_features/gpt2_last_layer.tsv',\n",
    "                   gpu=False, layer='intermidiate', layer_num=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2 4 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [03:23<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/gpt_4_last_layers.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt2_representation(data=prepared_annotations, model_name='gpt2-medium', path='../text_features/gpt_4_last_layers.tsv',\n",
    "                   gpu=False, layer='sum', num_last_layers_to_use=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stbs bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Version' object has no attribute 'major'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbcd88385343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentence-transformers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tokenizers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# must be loaded here, or else tqdm check may fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_torch_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mtorch_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     _torch_fx_available = (torch_version.major, torch_version.minor) == (\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mTORCH_FX_REQUIRED_VERSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mTORCH_FX_REQUIRED_VERSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Version' object has no attribute 'major'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stbs_representation(annotations, path, gpu=False):\n",
    "    stbs_bert = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    if gpu:\n",
    "        stbs_bert.to('gpu')\n",
    "    else:\n",
    "        stbs_bert.to('cpu')\n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for annotation in tqdm(annotations):\n",
    "            feature = stbs_bert.encode(annotation)\n",
    "            feature = np.ravel(feature)\n",
    "            csv_writer.writerow(feature)        \n",
    "    print('Saved representation to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [00:23<00:00, 164.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/stbs_bert.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stbs_representation(annotations,'../text_features/stbs_bert.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2062 [00:00<?, ?it/s]/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  import sys\n",
      "100%|██████████| 2062/2062 [00:00<00:00, 175006.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 words where absent in embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embeddings = get_emb_rep(unique_words_annotations, load_emb_from_disk('../embeddings/GoogleNews-vectors-negative300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/w2v.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_embeddings(w2v_embeddings, prepared_annotations, '../text_features/w2v.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_embeddings\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove('../embeddings/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2062/2062 [00:00<00:00, 796377.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 words where absent in embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove = get_emb_rep_glove(unique_words_annotations, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/glove.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_embeddings(glove,prepared_annotations, '../text_features/glove.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del glove\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2062 [00:00<?, ?it/s]/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  import sys\n",
      "100%|██████████| 2062/2062 [00:00<00:00, 208150.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 words where absent in embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fasttext_embs = get_emb_rep(unique_words_annotations,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/fasttext.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_embeddings(fasttext_embs,prepared_annotations, '../text_features/fasttext.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resNet50 on FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../weights/ResNet-50_faces.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(model.layers[0].input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images_fer(images):\n",
    "    images = (images*255).astype(np.uint8)\n",
    "    fer_images = []\n",
    "    for image in images:\n",
    "        fer_images.append(np.array(Image.fromarray(image).convert('RGB').resize((197, 197), Image.ANTIALIAS)))\n",
    "    return np.array(fer_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_fer = prepare_images_fer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [03:55<00:00, 16.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/resnet_fer_last.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images_fer,model,'../image_features/resnet_fer_last.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert  last 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [01:54<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/bert_4_last_layers.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_representation(data=prepared_annotations, model_name='bert-base-uncased', path='../text_features/bert_4_last_layers.tsv',\n",
    "                   gpu=False, layer='sum', num_last_layers_to_use=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [00:02<00:00, 1809.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/bert_emb.tsv\n"
     ]
    }
   ],
   "source": [
    "bert_representation(data=prepared_annotations, model_name='bert-base-uncased', path='../text_features/bert_emb.tsv',\n",
    "                   gpu=False, layer='emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [01:56<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/bert_last_layer.tsv\n"
     ]
    }
   ],
   "source": [
    "bert_representation(data=prepared_annotations, model_name='bert-base-uncased', path='../text_features/bert_last_layer.tsv',\n",
    "                   gpu=False, layer='intermidiate', layer_num=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/3'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/elmo/3: 180.34MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/elmo/3: 310.34MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/elmo/3, Total size: 357.40MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/elmo/3'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "100%|██████████| 3840/3840 [03:03<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/elmo.tsv\n"
     ]
    }
   ],
   "source": [
    "elmo_representation(data=annotations, path='../text_features/elmo.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient net on Age/Gender data  block3c_expand_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_efficient_net(path, layer='block3c_expand_conv'):\n",
    "    model = load_model(path)\n",
    "    inputs = model.layers[0]\n",
    "    last_conv = [i for i in model.layers if i.name==layer][0]\n",
    "    x = GlobalAveragePooling2D()(last_conv.output)\n",
    "    model = tf.keras.Model(inputs.input,x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_efficient_net('../weights/EfficientNetB3_224_weights.11-3.44.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [02:46<00:00, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/efnet_bloc3c_age_gender.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, model, '../image_features/efnet_bloc3c_age_gender.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient net on Age/Gender data  top_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_efficient_net('../weights/EfficientNetB3_224_weights.11-3.44.hdf5', 'top_activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [04:41<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/efnet_top_conv_age_gender.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, model, '../image_features/efnet_top_conv_age_gender.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchy-based Image Embeddings for Semantic Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../weights/imagenet_unitsphere-embed+cls_rn50.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = model.layers[0].input\n",
    "last_layer = [i for i in model.layers if i.name=='embedding'][0]\n",
    "model = keras.Model(inputs, last_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3840/3840 [01:40<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/hierarchy_based_sim.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, model, '../image_features/hierarchy_based_sim.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
