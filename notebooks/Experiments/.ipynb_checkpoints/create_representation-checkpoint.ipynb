{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing import image as im\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow_text as text\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.applications as apps\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-officer",
   "metadata": {},
   "source": [
    "# Loading data and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "following-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ABS_PATH):\n",
    "    annotations = [i for i in os.listdir(ABS_PATH) if i.endswith('csv')]\n",
    "    folders = [i for i in os.listdir(ABS_PATH) if not i.endswith('csv')]\n",
    "    df = pd.DataFrame()\n",
    "    for path in annotations:\n",
    "        df = pd.concat([df, pd.read_csv(os.path.join(ABS_PATH,path))], axis=0)\n",
    "    df = df.drop_duplicates('image_name').drop(columns='Unnamed: 0').dropna(how='any')\n",
    "    for folder in folders:\n",
    "        path = os.path.join(ABS_PATH, folder)\n",
    "        in_folder = os.listdir(path)\n",
    "        df.loc[df['image_name'].isin(in_folder), 'image_name'] = \\\n",
    "        df.loc[df['image_name'].isin(in_folder), 'image_name'].apply(lambda x: os.path.join(path,x))\n",
    "    df = df[df['image_name'].apply(lambda x: 'data' in x.split('/'))]\n",
    "    images = []\n",
    "    for image_name in df['image_name'].values:\n",
    "        images.append(np.array(im.load_img(image_name, target_size=(224,224))))\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "    annotations = df['annotation'].str.lower().values\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scenic-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations = load_data(\"../../data/imdb/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-bunny",
   "metadata": {},
   "source": [
    "# Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latin-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_folder('../image_features')\n",
    "check_folder('../text_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "biological-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "front-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation_tensorflow(data, model, path, use_pca=False, n_components=128, batch_size=16):\n",
    "    if use_pca:\n",
    "        pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "        data = np.split(data,int(np.ceil(len(data) / batch_size)))\n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for sample in tqdm(data):\n",
    "                if use_pca:\n",
    "                    sample = tf.convert_to_tensor(sample)\n",
    "                    features = np.hstack(model(sample).numpy())\n",
    "                    pca.partial_fit(features)\n",
    "                    features = pca.transform(features)\n",
    "                    for feature in features:\n",
    "                        csv_writer.writerows(feature)\n",
    "                else:\n",
    "                    sample = np.expand_dims(sample,axis=0)\n",
    "                    feature = np.hstack(model(sample))\n",
    "                    csv_writer.writerow(feature)            \n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confirmed-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data, path):\n",
    "    with open(path,'w') as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        csv_writer.writerows(data)            \n",
    "    print('Saved representations to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mathematical-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet(block='conv4_block5_out', pooling=True):\n",
    "    resnet = apps.ResNet152V2(include_top=False, weights='imagenet')\n",
    "    outputs = [i for i in resnet.layers if i.name==block][0]\n",
    "    inputs = resnet.layers[0]\n",
    "    if pooling:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(outputs.output)\n",
    "    else:\n",
    "        x = tf.keras.layers.Flatten()(outputs.output)\n",
    "    resnet = tf.keras.Model(inputs.input,x)\n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-legislation",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = load_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vertical-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [02:17<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../image_features/resnet_conv4_block5.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_representation_tensorflow(images, resnet, '../image_features/resnet_conv4_block5.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dense-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del resnet\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-geneva",
   "metadata": {},
   "source": [
    "# stbs bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spare-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prescribed-concentration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representations to : ../text_features/stbs_bert.tsv\n"
     ]
    }
   ],
   "source": [
    "stbs_bert = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "features = stbs_bert.encode(annotations)\n",
    "write(features, '../text_features/stbs_bert.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "round-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "del features, stbs_bert\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "paperback-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_from_disk(path):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worse-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "def prepare_annotations_embeddings(annotations):\n",
    "    prepared_annotations = []\n",
    "    for sentence in annotations:\n",
    "        sentence = ''.join([i for i in sentence if not (i in [',','.','!','?'] )])\n",
    "        prepared_annotations.append([i for i in sentence.split(' ') if not (i in stopwords.words('english'))])\n",
    "    unique_words_annotations = np.unique(np.hstack(prepared_annotations))\n",
    "    return prepared_annotations, unique_words_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bound-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_rep(tokens, embeddings):\n",
    "    dict_tokens = {}\n",
    "    missing = []\n",
    "    for w in tqdm(tokens):\n",
    "        try:\n",
    "            try:\n",
    "                dict_tokens.update({w: embeddings.word_vec(w.lower())})\n",
    "            except:\n",
    "                dict_tokens.update({w: embeddings.word_vec(w.captialize())})\n",
    "        except:\n",
    "            missing.append(w)\n",
    "\n",
    "    print('{} words where absent in embedding'.format(len(missing)))\n",
    "    return dict_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wicked-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation_embeddings(embeddings, sentences, path, dim=300):\n",
    "    with open(path, \"w\") as fw:\n",
    "        csv_writer = csv.writer(fw, delimiter='\\t')\n",
    "        for n, sentence in enumerate(sentences):\n",
    "            vector = np.zeros(shape=(dim,))\n",
    "            counter = 0 \n",
    "            for word in sentence:\n",
    "                representation = embeddings.get(word)\n",
    "                if not(representation is None):\n",
    "                    vector+=representation\n",
    "                    counter+=1\n",
    "            if counter!=0:\n",
    "                vector/=counter\n",
    "            csv_writer.writerow(vector)\n",
    "    print('Saved representation to : {}'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-choice",
   "metadata": {},
   "source": [
    "# w2v embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "specialized-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_annotations, unique_words_annotations = prepare_annotations_embeddings(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "awful-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1446 [00:00<?, ?it/s]/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  import sys\n",
      "100%|██████████| 1446/1446 [00:00<00:00, 311599.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 words where absent in embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embeddings = get_emb_rep(unique_words_annotations, load_emb_from_disk('../embeddings/GoogleNews-vectors-negative300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hundred-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/w2v.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_embeddings(w2v_embeddings, prepared_annotations, '../text_features/w2v.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "celtic-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_embeddings\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-calibration",
   "metadata": {},
   "source": [
    "# glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "engaged-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_rep_glove(tokens, embeddings):\n",
    "    dict_tokens = {}\n",
    "    missing = []\n",
    "    for w in tqdm(tokens):\n",
    "        if w in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w]})\n",
    "        elif w.lower() in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w.lower()]})\n",
    "        elif w.capitalize() in embeddings.keys():\n",
    "            dict_tokens.update({w: embeddings[w.capitalize()]})\n",
    "        else:\n",
    "            missing.append(w)\n",
    "\n",
    "    print('{} words where absent in embedding'.format(len(missing)))\n",
    "    return dict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "developmental-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(file, 'r')\n",
    "    glove_embeddings = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        glove_embeddings[word] = wordEmbedding\n",
    "    print(len(glove_embeddings), \" words loaded!\")\n",
    "    return glove_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "checked-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove = load_glove('../embeddings/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "silver-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1446/1446 [00:00<00:00, 897980.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 words where absent in embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove = get_emb_rep_glove(unique_words_annotations, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sorted-external",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved representation to : ../text_features/glove.tsv\n"
     ]
    }
   ],
   "source": [
    "create_representation_embeddings(glove,prepared_annotations, '../text_features/glove.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "australian-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "del glove\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-likelihood",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "refined-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capital-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc2vec(path_model, path_syn0, path_syn1):\n",
    "    model = gensim.models.Doc2Vec.load(path_model)\n",
    "    syn0 = np.array(np.load(path_syn0, mmap_mode='r'))\n",
    "    syn1 = np.array(np.load(path_syn1, mmap_mode='r'))\n",
    "    model.syn1 = syn1\n",
    "    model.syn0 = syn0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threaded-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = load_doc2vec('../embeddings/enwiki_dbow/doc2vec.bin','../embeddings/enwiki_dbow/doc2vec.bin.syn0.npy',\n",
    "            '../embeddings/enwiki_dbow/doc2vec.bin.syn1neg.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-cherry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
