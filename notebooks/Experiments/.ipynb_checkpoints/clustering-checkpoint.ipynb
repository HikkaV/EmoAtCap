{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5834130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from keras_preprocessing import image as im\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5988d05",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7275ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_path, annotations_path):\n",
    "    df = pd.read_csv(annotations_path)\n",
    "    df = df[~df['human_sentiment'].isna()]\n",
    "    in_folder = os.listdir(image_path)\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'] = \\\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'].apply(lambda x: os.path.join(image_path,x))\n",
    "    images = []\n",
    "    for image_name in df['image_name'].values:\n",
    "        images.append(np.array(im.load_img(image_name, target_size=(224,224))))\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "    annotations = df['annotation'].str.lower().values\n",
    "    return images, annotations, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a315e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inertia_plot(data,number=20):\n",
    "    inertia_list = []\n",
    "    list_clusters = []\n",
    "    for i in range(3,number+1):\n",
    "        kmeans = KMeans(n_clusters=i).fit(data)\n",
    "        inertia_list.append(kmeans.inertia_)\n",
    "        list_clusters.append(i)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Elbow method')\n",
    "    plt.scatter(list_clusters,inertia_list,s=50, color='r')\n",
    "    plt.plot(list_clusters,inertia_list)\n",
    "    plt.xticks(range(1,number+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa34ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_annotations(annotations, images, labels, n_labels=3, n_annotations=5):\n",
    "    unique_labels = np.random.choice(list(set(labels)),n_labels,replace=False)\n",
    "    for label in unique_labels:\n",
    "        idx  = np.hstack(np.where(labels==label))\n",
    "        tmp_annotations = np.random.choice(annotations[idx], n_annotations,replace=False)\n",
    "        for annotation in tmp_annotations:\n",
    "            tmp_idx = np.hstack(np.where(annotations==annotation))\n",
    "            plt.figure() \n",
    "            plt.imshow(images[tmp_idx][0])\n",
    "            plt.title('Annotations for label {} :'.format(label))\n",
    "            plt.grid(False)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.xlabel(annotation)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bece1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_images(images, labels, n_labels=3, n_images=5):\n",
    "    unique_labels = np.random.choice(list(set(labels)),size=n_labels,replace=False)\n",
    "    rows = len(unique_labels)\n",
    "    fig, axes = plt.subplots(rows, n_images, figsize=(20,20))\n",
    "    for c1, label in enumerate(unique_labels):\n",
    "        idx = np.hstack(np.where(labels==label))\n",
    "        tmp_images = images[idx]\n",
    "        to_show = tmp_images[np.random.choice(range(len(tmp_images)), size=n_images,replace=False)]\n",
    "        for c2, image in enumerate(to_show):\n",
    "            axes[c1][c2].imshow(image)\n",
    "            axes[c1][c2].set_ylabel(label)\n",
    "            axes[c1][c2].grid(False)\n",
    "            axes[c1][c2].set_yticklabels([])\n",
    "            axes[c1][c2].set_xticklabels([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ddb211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(path):\n",
    "    features = []\n",
    "    with open(path, 'r') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in csv_reader:\n",
    "            features.append(row)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9347df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(res1,res2):\n",
    "    adi = adjusted_rand_score(res1, res2)*100\n",
    "    nmi = normalized_mutual_info_score(res1, res2)*100\n",
    "    return adi, nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2248c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(features, n_clusters=9, pca=False, variance=0.9):\n",
    "    if isinstance(features, str):\n",
    "        features = read_features(features)\n",
    "    if pca:\n",
    "        features = PCA(variance).fit_transform(features)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=1000, random_state=0)\n",
    "    kmeans = kmeans.fit(features)\n",
    "    predictions = kmeans.predict(features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "780a66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_clustering(df, path_feature, pca=False, variance=0.9):\n",
    "    n_clusters = df['correct_sentiment'].nunique()\n",
    "    predictions = cluster(path_feature, n_clusters,pca, variance)\n",
    "    return metrics(predictions, df['correct_sentiment'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac7ebc",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10e7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations, df = load_data(\"../../data/emo-at-cap/images/\", '../../data/emo-at-cap/emo-at-cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f5ad75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fb14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_sentiments = dict([(i,c) for c,i in enumerate(df['human_sentiment'].unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c29154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct_sentiment'] = df['human_sentiment'].map(mapping_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccac4d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>annotation</th>\n",
       "      <th>human_sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>correct_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/emo-at-cap/images/nm1055413_rm70404...</td>\n",
       "      <td>The man is running from something</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/emo-at-cap/images/nm0000113_rm13102...</td>\n",
       "      <td>The worried woman is carrying for the other woman</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/emo-at-cap/images/nm0001713_rm27135...</td>\n",
       "      <td>The man and the woman are trying to protect th...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/emo-at-cap/images/nm4237013_rm66314...</td>\n",
       "      <td>The man is flirting with the woman</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/emo-at-cap/images/nm0000113_rm13502...</td>\n",
       "      <td>The man and the woman are surprised by something</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  \\\n",
       "0  ../../data/emo-at-cap/images/nm1055413_rm70404...   \n",
       "1  ../../data/emo-at-cap/images/nm0000113_rm13102...   \n",
       "2  ../../data/emo-at-cap/images/nm0001713_rm27135...   \n",
       "3  ../../data/emo-at-cap/images/nm4237013_rm66314...   \n",
       "4  ../../data/emo-at-cap/images/nm0000113_rm13502...   \n",
       "\n",
       "                                          annotation human_sentiment  \\\n",
       "0                  The man is running from something         Neutral   \n",
       "1  The worried woman is carrying for the other woman        Positive   \n",
       "2  The man and the woman are trying to protect th...        Positive   \n",
       "3                 The man is flirting with the woman        Positive   \n",
       "4   The man and the woman are surprised by something        Positive   \n",
       "\n",
       "  sentiment  correct_sentiment  \n",
       "0   Neutral                  0  \n",
       "1  Negative                  1  \n",
       "2  Positive                  1  \n",
       "3  Positive                  1  \n",
       "4  Positive                  1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090e6563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55253b4",
   "metadata": {},
   "source": [
    "# Clustering with respect to sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824f97d",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_sentiment_table = []\n",
    "for text_feature_name in os.listdir('../text_features/'):\n",
    "    adi, nmi = sentiment_clustering(df, os.path.join('../text_features/',text_feature_name))\n",
    "    text_features_sentiment_table.append((text_feature_name.split('.')[0], adi, nmi))\n",
    "text_features_sentiment_table = pd.DataFrame(text_features_sentiment_table, columns=['representation',\n",
    "                                                                                     'adi','nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f69c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_sentiment_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491aa215",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_sentiment_table = []\n",
    "for image_feature_name in os.listdir('../image_features/'):\n",
    "    adi, nmi = sentiment_clustering(df, os.path.join('../image_features/',image_feature_name))\n",
    "    image_features_sentiment_table.append((image_feature_name.split('.')[0], adi, nmi))\n",
    "image_features_sentiment_table = pd.DataFrame(image_features_sentiment_table, columns=['image_model','adi','nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0221bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_sentiment_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_sentiment_table.to_csv('sentiment_clustering_text.csv', index=False)\n",
    "image_features_sentiment_table.to_csv('sentiment_clustering_images.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b03b0b",
   "metadata": {},
   "source": [
    "## trying out PCA to get rid of noise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553f69f",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107aecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features_sentiment_table_pca = []\n",
    "for text_feature_name in os.listdir('../text_features/'):\n",
    "    adi, nmi = sentiment_clustering(df, os.path.join('../text_features/',text_feature_name), pca=True, variance=0.9)\n",
    "    text_features_sentiment_table_pca.append((text_feature_name.split('.')[0], adi, nmi))\n",
    "text_features_sentiment_table_pca = pd.DataFrame(text_features_sentiment_table_pca, columns=['representation',\n",
    "                                                                                     'adi','nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([text_features_sentiment_table,text_features_sentiment_table_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a5a3",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c56c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_sentiment_table_pca = []\n",
    "for image_feature_name in os.listdir('../image_features/'):\n",
    "    adi, nmi = sentiment_clustering(df, os.path.join('../image_features/',image_feature_name), pca=True, variance=0.9)\n",
    "    image_features_sentiment_table_pca.append((image_feature_name.split('.')[0], adi, nmi))\n",
    "image_features_sentiment_table_pca = pd.DataFrame(image_features_sentiment_table_pca, columns=['image_model','adi','nmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a82aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([image_features_sentiment_table,image_features_sentiment_table_pca],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f6121",
   "metadata": {},
   "source": [
    "# Clustering with KMeans to determine understanding between images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_clusters(features, k=(1,12)):\n",
    "    visualizer = KElbowVisualizer(KMeans(), k=k)\n",
    "    visualizer.fit(features.astype(np.float64))\n",
    "    return visualizer.elbow_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterimne_clusters(features1, features2):\n",
    "    n_clusters1 = get_n_clusters(features1)\n",
    "    n_clusters2 = get_n_clusters(features2)\n",
    "    return np.floor((n_clusters1+n_clusters2)/2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_images_clustering():\n",
    "    list_features_text = os.listdir('../text_features/')\n",
    "    list_features_images = os.listdir('../image_features/')\n",
    "    results = []\n",
    "    for feature_text in tqdm(list_features_text):\n",
    "        name_feature_text = feature_text.split('.')[0]\n",
    "        feature_text = read_features(os.path.join('../text_features/', feature_text))\n",
    "        for feature_image in tqdm(list_features_images):\n",
    "            name_feature_image = feature_image.split('.')[0]\n",
    "            feature_image = read_features(os.path.join('../image_features/', feature_image))\n",
    "            k = deterimne_clusters(feature_text,feature_image)\n",
    "            adi, nmi = metrics(cluster(feature_image, n_clusters=k),cluster(feature_text, n_clusters=k))\n",
    "            results.append((name_feature_text,name_feature_image, adi, nmi, k))\n",
    "    return pd.DataFrame(results, columns=['representation_text','representation_images','adi','nmi','n_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_images_table = text_images_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38130b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_images_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d572fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_images_table['n_clusters'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_images_table[text_images_table.nmi==text_images_table.nmi.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a230a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adi = pd.pivot_table(text_images_table, values='adi',index=text_images_table['representation_text'], columns=text_images_table['representation_images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7aed639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>representation_images</th>\n",
       "      <th>efnet_bloc3c_age_gender</th>\n",
       "      <th>efnet_top_conv_age_gender</th>\n",
       "      <th>hierarchy_based_sim</th>\n",
       "      <th>resnet_conv4_block5</th>\n",
       "      <th>resnet_conv5_block3</th>\n",
       "      <th>resnet_fer_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>representation_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert_4_last_layers</th>\n",
       "      <td>0.833623</td>\n",
       "      <td>0.884147</td>\n",
       "      <td>0.194743</td>\n",
       "      <td>1.134678</td>\n",
       "      <td>1.920004</td>\n",
       "      <td>0.392439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_emb</th>\n",
       "      <td>1.186559</td>\n",
       "      <td>1.055698</td>\n",
       "      <td>0.261113</td>\n",
       "      <td>1.142026</td>\n",
       "      <td>2.122105</td>\n",
       "      <td>0.222157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert_last_layer</th>\n",
       "      <td>0.430127</td>\n",
       "      <td>0.628109</td>\n",
       "      <td>0.312184</td>\n",
       "      <td>0.725266</td>\n",
       "      <td>1.639509</td>\n",
       "      <td>0.561105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo</th>\n",
       "      <td>1.757876</td>\n",
       "      <td>1.296754</td>\n",
       "      <td>0.425018</td>\n",
       "      <td>1.880216</td>\n",
       "      <td>2.590038</td>\n",
       "      <td>0.156094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fasttext</th>\n",
       "      <td>2.498562</td>\n",
       "      <td>1.802226</td>\n",
       "      <td>0.242030</td>\n",
       "      <td>1.773613</td>\n",
       "      <td>2.444834</td>\n",
       "      <td>0.187511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove</th>\n",
       "      <td>2.504884</td>\n",
       "      <td>1.673156</td>\n",
       "      <td>0.397759</td>\n",
       "      <td>2.320899</td>\n",
       "      <td>2.974737</td>\n",
       "      <td>0.180679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_11th_layer</th>\n",
       "      <td>1.823289</td>\n",
       "      <td>1.422065</td>\n",
       "      <td>0.093339</td>\n",
       "      <td>2.040109</td>\n",
       "      <td>1.824051</td>\n",
       "      <td>0.076315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_last_layer</th>\n",
       "      <td>0.365918</td>\n",
       "      <td>0.304745</td>\n",
       "      <td>0.121246</td>\n",
       "      <td>0.768297</td>\n",
       "      <td>0.455685</td>\n",
       "      <td>0.310785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2_wte</th>\n",
       "      <td>1.096811</td>\n",
       "      <td>0.896934</td>\n",
       "      <td>0.225107</td>\n",
       "      <td>1.420297</td>\n",
       "      <td>1.696157</td>\n",
       "      <td>0.138305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4_last_layers</th>\n",
       "      <td>0.578583</td>\n",
       "      <td>0.522264</td>\n",
       "      <td>0.081558</td>\n",
       "      <td>0.350201</td>\n",
       "      <td>1.082370</td>\n",
       "      <td>0.141272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stbs_bert</th>\n",
       "      <td>0.180181</td>\n",
       "      <td>0.105220</td>\n",
       "      <td>0.586990</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.256400</td>\n",
       "      <td>0.832161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w2v</th>\n",
       "      <td>2.618769</td>\n",
       "      <td>2.033211</td>\n",
       "      <td>0.415345</td>\n",
       "      <td>2.385322</td>\n",
       "      <td>3.210155</td>\n",
       "      <td>0.218249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "representation_images  efnet_bloc3c_age_gender  efnet_top_conv_age_gender  \\\n",
       "representation_text                                                         \n",
       "bert_4_last_layers                    0.833623                   0.884147   \n",
       "bert_emb                              1.186559                   1.055698   \n",
       "bert_last_layer                       0.430127                   0.628109   \n",
       "elmo                                  1.757876                   1.296754   \n",
       "fasttext                              2.498562                   1.802226   \n",
       "glove                                 2.504884                   1.673156   \n",
       "gpt2_11th_layer                       1.823289                   1.422065   \n",
       "gpt2_last_layer                       0.365918                   0.304745   \n",
       "gpt2_wte                              1.096811                   0.896934   \n",
       "gpt_4_last_layers                     0.578583                   0.522264   \n",
       "stbs_bert                             0.180181                   0.105220   \n",
       "w2v                                   2.618769                   2.033211   \n",
       "\n",
       "representation_images  hierarchy_based_sim  resnet_conv4_block5  \\\n",
       "representation_text                                               \n",
       "bert_4_last_layers                0.194743             1.134678   \n",
       "bert_emb                          0.261113             1.142026   \n",
       "bert_last_layer                   0.312184             0.725266   \n",
       "elmo                              0.425018             1.880216   \n",
       "fasttext                          0.242030             1.773613   \n",
       "glove                             0.397759             2.320899   \n",
       "gpt2_11th_layer                   0.093339             2.040109   \n",
       "gpt2_last_layer                   0.121246             0.768297   \n",
       "gpt2_wte                          0.225107             1.420297   \n",
       "gpt_4_last_layers                 0.081558             0.350201   \n",
       "stbs_bert                         0.586990             1.333001   \n",
       "w2v                               0.415345             2.385322   \n",
       "\n",
       "representation_images  resnet_conv5_block3  resnet_fer_last  \n",
       "representation_text                                          \n",
       "bert_4_last_layers                1.920004         0.392439  \n",
       "bert_emb                          2.122105         0.222157  \n",
       "bert_last_layer                   1.639509         0.561105  \n",
       "elmo                              2.590038         0.156094  \n",
       "fasttext                          2.444834         0.187511  \n",
       "glove                             2.974737         0.180679  \n",
       "gpt2_11th_layer                   1.824051         0.076315  \n",
       "gpt2_last_layer                   0.455685         0.310785  \n",
       "gpt2_wte                          1.696157         0.138305  \n",
       "gpt_4_last_layers                 1.082370         0.141272  \n",
       "stbs_bert                         2.256400         0.832161  \n",
       "w2v                               3.210155         0.218249  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nmi = pd.pivot_table(text_images_table, values='nmi',index=text_images_table['representation_text'], columns=text_images_table['representation_images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adi.to_csv('correspondence_clustering_adi.csv',index=False)\n",
    "df_nmi.to_csv('correspondence_clustering_nmi.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
