{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "peaceful-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hollow-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from keras_preprocessing import image as im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-maintenance",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surface-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_path, annotations_path):\n",
    "    df = pd.read_csv(annotations_path)\n",
    "    df = df[~df['human_sentiment'].isna()]\n",
    "    in_folder = os.listdir(image_path)\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'] = \\\n",
    "    df.loc[df['image_name'].isin(in_folder), 'image_name'].apply(lambda x: os.path.join(image_path,x))\n",
    "    images = []\n",
    "    for image_name in df['image_name'].values:\n",
    "        images.append(np.array(im.load_img(image_name, target_size=(224,224))))\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "    annotations = df['annotation'].str.lower().values\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "shaped-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations = load_data(\"../../data/emo-at-cap/images/\", '../../data/emo-at-cap/emo-at-cap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-watch",
   "metadata": {},
   "source": [
    "# Processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quick-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inclusive-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "retired-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenizer.tokenize(i) for i in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens, unique_counts = np.unique(np.hstack(tokenized), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crucial-burton",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3261),\n",
       " ('is', 2031),\n",
       " ('man', 1905),\n",
       " ('and', 1536),\n",
       " ('woman', 1361),\n",
       " ('are', 1036),\n",
       " ('a', 963),\n",
       " ('to', 891),\n",
       " ('looks', 751),\n",
       " ('happy', 705),\n",
       " ('with', 547),\n",
       " ('something', 480),\n",
       " ('men', 478),\n",
       " ('of', 463),\n",
       " ('two', 408),\n",
       " ('other', 361),\n",
       " ('people', 343),\n",
       " ('couple', 334),\n",
       " (',', 330),\n",
       " ('in', 278),\n",
       " ('look', 270),\n",
       " ('because', 265),\n",
       " ('they', 256),\n",
       " ('about', 247),\n",
       " ('on', 246),\n",
       " ('at', 192),\n",
       " ('trying', 190),\n",
       " ('serious', 189),\n",
       " ('women', 184),\n",
       " ('each', 179),\n",
       " ('together', 176),\n",
       " ('by', 175),\n",
       " ('having', 171),\n",
       " ('flirting', 165),\n",
       " ('he', 155),\n",
       " ('smiling', 151),\n",
       " ('arguing', 141),\n",
       " ('angry', 140),\n",
       " ('scared', 138),\n",
       " ('worried', 137),\n",
       " ('hugging', 135),\n",
       " ('surprised', 134),\n",
       " ('calm', 133),\n",
       " ('company', 131),\n",
       " ('her', 129),\n",
       " ('group', 123),\n",
       " ('for', 121),\n",
       " ('she', 117),\n",
       " ('looking', 117),\n",
       " ('his', 115)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(unique_tokens, unique_counts)), key = lambda x: x[1])[::-1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olive-imaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "golden-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '<PAD>'\n",
    "start_token = '<S>'\n",
    "end_token = '<E>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "religious-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(zip(unique_tokens,list(range(3,len(unique_tokens)+3))))\n",
    "vocab[pad_token] = 0\n",
    "vocab[start_token] = 1\n",
    "vocab[end_token] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sexual-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = dict([(v,k) for k,v in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "foster-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_special_tokens = lambda x: [start_token] + x + [end_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "material-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = list(map(add_special_tokens,tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "combined-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(max(tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prescribed-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = list(map(lambda x: [vocab[i] for i in x],tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ultimate-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = tf.keras.preprocessing.sequence.pad_sequences(indexed, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instructional-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3840, 224, 224, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-piano",
   "metadata": {},
   "source": [
    "# Simple model without additional features, transfer learning and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "manufactured-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(x, f, s, padding='same'):\n",
    "    p = 2 if padding=='same' else 1\n",
    "    return (x-f+2)/s+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-literature",
   "metadata": {},
   "source": [
    "### Convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ruled-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(tf.keras.Model):\n",
    "    def __init__(self, image_shape=(224,224,3)):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.input_conv = tf.keras.layers.Conv2D(filters=128, kernel_size=7, input_shape=image_shape, activation='relu',\n",
    "                           padding='same', name='input_conv', strides=(1,1))\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation='relu',\n",
    "                               padding='same', name='conv1', strides=(2,2))\n",
    "        self.batch_norm1 =  tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, activation='relu',\n",
    "                               padding='same', name='conv2', strides=(2,2))\n",
    "        self.batch_norm2 =  tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu',\n",
    "                               padding='same', name='conv3', strides=(2,2))\n",
    "        self.batch_norm3 =  tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters=8, kernel_size=3, activation='relu',\n",
    "                               padding='same', name='conv4', strides=(2,2))\n",
    "        self.batch_norm4 =  tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv5 = tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='relu',\n",
    "                               padding='same', name='conv5', strides=(2,2))\n",
    "        self.batch_norm5 =  tf.keras.layers.BatchNormalization()\n",
    "        self.conv6 = tf.keras.layers.Conv2D(filters=4, kernel_size=3, activation='relu',\n",
    "                               padding='same', name='conv5', strides=(2,2))\n",
    "        self.batch_norm6 =  tf.keras.layers.BatchNormalization()\n",
    "       \n",
    "        self.flatten = tf.keras.layers.Flatten(name='final_code')\n",
    "    \n",
    "    def call(self, input):\n",
    "        conv1_out = self.batch_norm1(self.conv1(self.input_conv(input)))\n",
    "        conv2_out = self.batch_norm2(self.conv2(conv1_out))\n",
    "        conv3_out = self.batch_norm3(self.conv3(conv2_out))\n",
    "        conv4_out = self.batch_norm4(self.conv4(conv3_out))\n",
    "        conv5_out = self.batch_norm5(self.conv5(conv4_out))\n",
    "        conv6_out = self.batch_norm6(self.conv6(conv4_out))\n",
    "        result = [self.flatten(conv5_out),self.flatten(conv6_out)]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "naughty-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder = ConvEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "phantom-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv_encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_res = conv_encoder(np.expand_dims(images[0],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "horizontal-kruger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=500, shape=(1, 196), dtype=float32, numpy=\n",
       " array([[0.00525026, 0.        , 0.00083042, 0.        , 0.00241793,\n",
       "         0.        , 0.00110817, 0.        , 0.0006188 , 0.        ,\n",
       "         0.        , 0.        , 0.00089579, 0.        , 0.        ,\n",
       "         0.        , 0.00647726, 0.        , 0.00365794, 0.        ,\n",
       "         0.00491541, 0.00151088, 0.00660368, 0.        , 0.00187089,\n",
       "         0.        , 0.00122871, 0.        , 0.00706915, 0.        ,\n",
       "         0.        , 0.        , 0.00394136, 0.        , 0.00492814,\n",
       "         0.        , 0.00394737, 0.00301545, 0.00799373, 0.        ,\n",
       "         0.00811646, 0.        , 0.00660611, 0.        , 0.01675731,\n",
       "         0.        , 0.00997942, 0.        , 0.00417852, 0.00534826,\n",
       "         0.00444475, 0.        , 0.        , 0.        , 0.0020483 ,\n",
       "         0.        , 0.00427451, 0.        , 0.00688476, 0.        ,\n",
       "         0.00200678, 0.00571699, 0.00622021, 0.        , 0.01912067,\n",
       "         0.        , 0.00326044, 0.        , 0.02427681, 0.        ,\n",
       "         0.0036176 , 0.        , 0.00935859, 0.00342718, 0.00767713,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.00689524,\n",
       "         0.        , 0.00197751, 0.        , 0.00745501, 0.        ,\n",
       "         0.00146245, 0.        , 0.01433566, 0.        , 0.00243102,\n",
       "         0.        , 0.01537036, 0.        , 0.00460693, 0.        ,\n",
       "         0.02332996, 0.00204689, 0.00698033, 0.        , 0.02041095,\n",
       "         0.0006896 , 0.        , 0.        , 0.00358139, 0.00271245,\n",
       "         0.00419844, 0.        , 0.00800377, 0.00332257, 0.        ,\n",
       "         0.        , 0.0118275 , 0.        , 0.00525904, 0.        ,\n",
       "         0.00685166, 0.00455952, 0.00080342, 0.        , 0.01217279,\n",
       "         0.        , 0.00461601, 0.        , 0.01566431, 0.        ,\n",
       "         0.00795246, 0.00591378, 0.00488328, 0.        , 0.00834632,\n",
       "         0.        , 0.00909521, 0.        , 0.00760974, 0.00088355,\n",
       "         0.00596507, 0.00365129, 0.        , 0.        , 0.00708289,\n",
       "         0.00043756, 0.00329668, 0.        , 0.01617148, 0.        ,\n",
       "         0.00129148, 0.        , 0.00954282, 0.        , 0.        ,\n",
       "         0.        , 0.00756371, 0.00179973, 0.00494738, 0.        ,\n",
       "         0.00745154, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.0013442 , 0.        , 0.01924292, 0.00104019,\n",
       "         0.        , 0.        , 0.01782453, 0.00095739, 0.        ,\n",
       "         0.        , 0.02038948, 0.        , 0.        , 0.        ,\n",
       "         0.01631915, 0.        , 0.        , 0.        , 0.01560105,\n",
       "         0.        , 0.        , 0.        , 0.02685989, 0.00136882,\n",
       "         0.        , 0.        , 0.01212207, 0.00015704, 0.        ,\n",
       "         0.        ]], dtype=float32)>,\n",
       " <tf.Tensor: id=502, shape=(1, 196), dtype=float32, numpy=\n",
       " array([[0.        , 0.        , 0.        , 0.00913325, 0.        ,\n",
       "         0.        , 0.        , 0.00777858, 0.        , 0.        ,\n",
       "         0.        , 0.00690656, 0.        , 0.        , 0.        ,\n",
       "         0.0090482 , 0.        , 0.        , 0.        , 0.00324875,\n",
       "         0.        , 0.        , 0.        , 0.00241746, 0.        ,\n",
       "         0.        , 0.        , 0.00023418, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.00432075, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.0011404 , 0.        ,\n",
       "         0.        , 0.        , 0.01044512, 0.        , 0.        ,\n",
       "         0.        , 0.00016562, 0.        , 0.        , 0.        ,\n",
       "         0.00461862, 0.        , 0.        , 0.00713086, 0.00487771,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.00745952, 0.00475232, 0.        ,\n",
       "         0.        , 0.0071784 , 0.        , 0.00688455, 0.        ,\n",
       "         0.        , 0.        , 0.00042192, 0.        , 0.00965258,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.0058204 , 0.        , 0.        ,\n",
       "         0.        , 0.006005  , 0.        , 0.        , 0.        ,\n",
       "         0.00160827, 0.        , 0.        , 0.        , 0.01693405,\n",
       "         0.        , 0.        , 0.        , 0.01058836, 0.        ,\n",
       "         0.        , 0.        , 0.01263346, 0.        , 0.        ,\n",
       "         0.00402313, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.00048725, 0.        , 0.        , 0.        , 0.00445134,\n",
       "         0.        , 0.        , 0.        , 0.00248379, 0.        ,\n",
       "         0.        , 0.        , 0.00520054, 0.        , 0.        ,\n",
       "         0.        , 0.01374377, 0.        , 0.        , 0.        ,\n",
       "         0.00863237, 0.00150158, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.00458011, 0.        ,\n",
       "         0.        , 0.        , 0.00533952, 0.        , 0.        ,\n",
       "         0.        , 0.00870073, 0.        , 0.        , 0.        ,\n",
       "         0.00609551, 0.        , 0.00350535, 0.        , 0.0020485 ,\n",
       "         0.        , 0.        , 0.        , 0.00256764, 0.        ,\n",
       "         0.        , 0.        , 0.00828033, 0.        , 0.        ,\n",
       "         0.        , 0.00491805, 0.        , 0.00065213, 0.        ,\n",
       "         0.        , 0.        , 0.00395676, 0.        , 0.00379323,\n",
       "         0.        , 0.001805  , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.00215189, 0.        , 0.00512455,\n",
       "         0.        , 0.00275178, 0.        , 0.00595794, 0.        ,\n",
       "         0.        ]], dtype=float32)>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-federation",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "through-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(tf.keras.Model):\n",
    "    def __init__(self, max_tokens, lstm_units):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embeddings = tf.keras.layers.Embedding(input_dim=max_tokens+1, output_dim=128, name='embeddings')\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layer_normalization')\n",
    "        self.lstm = tf.keras.layers.LSTM(units=lstm_units, return_state=True, name='lstm_decoder')\n",
    "        self.output_dense = tf.keras.layers.Dense(units=max_tokens)\n",
    "        \n",
    "    def call(self, input, features):\n",
    "        embedded = self.embeddings(input)\n",
    "        result_lstm, state_h, state_c = self.lstm(embedded, initial_state=features)\n",
    "        normalized = self.layer_norm(result_lstm)\n",
    "        logits = self.output_dense(normalized)\n",
    "        return logits, [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "other-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_decoder = LSTMDecoder(len(inverse_vocab), conv_res[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "communist-credits",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 196, 1, 37, 1, 196]  [Op:CudnnRNN]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e84fecd11837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7fcaa1604d7b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input, features)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mresult_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m   def call(self,\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m           last_output, outputs, new_h, new_c, runtime = cudnn_lstm(\n\u001b[0;32m--> 961\u001b[0;31m               **cudnn_lstm_kwargs)\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcudnn_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n\u001b[1;32m   1173\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         rnn_mode='lstm')\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m   \u001b[0mlast_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0minput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             ctx=_ctx)\n\u001b[0m\u001b[1;32m    110\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn_eager_fallback\u001b[0;34m(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\u001b[0m\n\u001b[1;32m    196\u001b[0m   \"is_training\", is_training)\n\u001b[1;32m    197\u001b[0m   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\n\u001b[0;32m--> 198\u001b[0;31m                              attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    199\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    200\u001b[0m       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/home/volodymyr/Parallel-emotional-intent-clustering/env/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 196, 1, 37, 1, 196]  [Op:CudnnRNN]"
     ]
    }
   ],
   "source": [
    "lstm_decoder(np.expand_dims(padded[0],axis=0), conv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-rugby",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noted-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dense-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none',\n",
    "                                                                         from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fixed-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, vocab[pad_token]))\n",
    "        loss_ = loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "patient-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, target, lengths, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        initial_state = conv_encoder(image)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        batched_loss = []\n",
    "        for t in range(1, target.shape[1]):\n",
    "            \n",
    "            dec_input = tf.expand_dims(target[:, t-1], 1)\n",
    "            result, initial_state = lstm_decoder(input=dec_input, features=initial_state)\n",
    "\n",
    "            batched_loss.append(loss_function(target[:, t], result))\n",
    "        batched_loss = tf.reshape(tf.stack(batched_loss), shape=target[:,1:].shape)\n",
    "        batched_loss = tf.reduce_sum(batched_loss, axis=1)\n",
    "        lengths = tf.cast(lengths, dtype=batched_loss.dtype)\n",
    "        loss = tf.reduce_mean(batched_loss / lengths)\n",
    "\n",
    "    perplexity = tf.exp(loss)\n",
    "\n",
    "    variables = conv_encoder.trainable_variables + lstm_decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pointed-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate(image, target, lengths, optimizer):\n",
    "\n",
    "    initial_state = conv_encoder(image)\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    batched_loss = []\n",
    "    for t in range(1, target.shape[1]):\n",
    "\n",
    "        dec_input = tf.expand_dims(target[:, t-1], 1)\n",
    "        result, initial_state = lstm_decoder(input=dec_input, features=initial_state)\n",
    "\n",
    "        batched_loss.append(loss_function(target[:, t], result))\n",
    "\n",
    "    batched_loss = tf.reshape(tf.stack(batched_loss), shape=target[:,1:].shape)\n",
    "    batched_loss = tf.reduce_sum(batched_loss, axis=1)\n",
    "    lengths = tf.cast(lengths, dtype=batched_loss.dtype)\n",
    "    loss = tf.reduce_mean(batched_loss / lengths)\n",
    "\n",
    "    perplexity = tf.exp(loss)\n",
    "\n",
    "\n",
    "    return loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prescribed-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_sequences, test_sequences =  train_test_split(images, padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "marine-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del images, padded;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "occupied-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = (train_sequences!=vocab[pad_token]).sum(axis=1)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bearing-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = (test_sequences!=vocab[pad_token]).sum(axis=1)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "applied-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size= 64\n",
    "train_images = np.array(np.array_split(train_images, len(train_images)//batch_size))\n",
    "test_images = np.array(np.array_split(test_images, len(test_images)//batch_size))\n",
    "train_sequences = np.array(np.array_split(train_sequences, len(train_sequences)//batch_size))\n",
    "test_sequences = np.array(np.array_split(test_sequences, len(test_sequences)//batch_size))\n",
    "train_length = np.array(np.array_split(train_length, len(train_length)//batch_size))\n",
    "test_length = np.array(np.array_split(test_length, len(test_length)//batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sublime-morris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54, 64, 224, 224, 3), (54, 64, 37), (54, 64))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_sequences.shape, train_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "auburn-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.68s/it]"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_perplexity = []\n",
    "test_loss = []\n",
    "test_perplexity = []\n",
    "for epoch in range(1):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_perplexity = 0\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_perplexity = 0\n",
    "    for batch_train_images, batch_train_sequences, batched_train_length in tqdm(zip(train_images,train_sequences,train_length)):\n",
    "        batched_train_loss, batched_train_perplexity = train_step(batch_train_images,batch_train_sequences,batched_train_length, optimizer)\n",
    "        epoch_train_loss+=batched_train_loss\n",
    "        epoch_train_perplexity+=batched_train_perplexity\n",
    "    epoch_train_loss = epoch_train_loss/batch_size\n",
    "    epoch_train_perplexity =epoch_train_perplexity/batch_size\n",
    "    print('Finished epoch {}; Train loss : {}; Train perplexity : {}'.format(epoch,epoch_train_loss,epoch_train_perplexity))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-lover",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
